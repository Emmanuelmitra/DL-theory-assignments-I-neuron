{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5h9EE7vIb6N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## dl theory assingment--***11***"
      ],
      "metadata": {
        "id": "9IIXEezUJCaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uhqqqRmrJBkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "To implement a single neuron in Python, you can create a class with a method for the forward pass, which takes in an input and applies a weight and bias before passing it through an activation function. The following is an example of a single neuron with a sigmoid activation function:"
      ],
      "metadata": {
        "id": "Js8mjVlFIfXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron:\n",
        "    def __init__(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        weighted_sum = sum(inputs * self.weights) + self.bias\n",
        "        return 1 / (1 + math.exp(-weighted_sum))\n"
      ],
      "metadata": {
        "id": "PYzezMDKIgbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CNXzUO5vIj-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.To implement the ReLU activation function in Python, you can create a function that takes in a single input and returns the maximum of that input and 0. Here is an example of a ReLU function:"
      ],
      "metadata": {
        "id": "R_ymuF40ImjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(inputs):\n",
        "    return max(inputs, 0)\n"
      ],
      "metadata": {
        "id": "G2cNkaTlIn6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "udcHVFcZIpqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.\n",
        "To implement a dense layer in terms of matrix multiplication, you can use the matmul() function in Python to multiply the input matrix by the weight matrix, and then add the bias. Here is an example of a dense layer with a ReLU activation function:"
      ],
      "metadata": {
        "id": "i1w8jJIgIqJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class DenseLayer:\n",
        "    def __init__(self, weight_matrix, bias):\n",
        "        self.weight_matrix = weight_matrix\n",
        "        self.bias = bias\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        weighted_sum = torch.matmul(inputs, self.weight_matrix) + self.bias\n",
        "        return torch.relu(weighted_sum)\n"
      ],
      "metadata": {
        "id": "Uaob9YqtIsXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.To implement a dense layer in plain Python, you can use list comprehensions and built-in functions to perform the matrix multiplication and bias addition. Here is an example of a dense layer with a ReLU activation function:"
      ],
      "metadata": {
        "id": "PWRuG2X1IwBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseLayer:\n",
        "    def __init__(self, weight_matrix, bias):\n",
        "        self.weight_matrix = weight_matrix\n",
        "        self.bias = bias\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        weighted_sum = [sum(inputs[i] * self.weight_matrix[i]) + self.bias for i in range(len(inputs))]\n",
        "        return [max(0, x) for x in weighted_sum]\n"
      ],
      "metadata": {
        "id": "BG1tYrLlIt7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Qk63ybwI00E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.\tThe \"hidden size\" of a layer refers to the number of neurons or units in a hidden layer of a neural network.\n",
        "6.\tThe t() method in PyTorch transposes a tensor, which means it swaps the rows and columns of the tensor.\n",
        "7.\tMatrix multiplication written in plain Python is slow because it requires nested loops to iterate over the elements of the matrices and perform the multiplication, which can be computationally expensive for large matrices.\n",
        "8.\tIn the matmul function, ac==br because it's a matrix multiplication, which is the product of matrix A and B, the number of columns of matrix A must equal the number of rows of matrix B.\n",
        "9.\tIn Jupyter Notebook, you can measure the time taken for a single cell to execute by adding the %timeit magic command before the code you want to measure.\n",
        "10.\tElementwise arithmetic refers to performing mathematical operations, such as addition or multiplication, on each element of an array or matrix individually.\n",
        "11.\tTo test whether every element of a is\n"
      ],
      "metadata": {
        "id": "UBxvFYBII3FJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eAZtLOrVI9dw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}